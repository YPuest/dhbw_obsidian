#Mathe #Mathe1 
PDFs: [[Kapitel 9 - Matrizen.pdf|Präsentation]] [[Kapitel 9 - Matrizen - Kommentierungen.pdf|Kommentierungen]] [[Kapitel 9 - Matrizen - Übungen_Lösungen.pdf|Übungen]]

## Themen
1. [[8. Matrizen#Definition und Grundoperationen|Definition und Grundoperationen]]
2. [[8. Matrizen#Multiplikation von Matrizen|Multiplikation von Matrizen]]
3. [[8. Matrizen#Matrizen für elementare Zeilenumformungen|Matrizen für elementare Zeilenumformungen]]
4. [[8. Matrizen#Transposition von Matrizen|Transposition von Matrizen]]
5. [[8. Matrizen#Inversion von quadratischen Matrizen|Inversion von quadratischen Matrizen]]
6. [[8. Matrizen#Orthogonalität von Matrizen|Orthogonalität von Matrizen]]
7. [[8. Matrizen#Abbildungen von quadratischen Matrizen|Abbildungen von quadratischen Matrizen]]
8. [[8. Matrizen#Matrix-Normen|Matrix-Normen]]

---

## Definition und Grundoperationen

```ad-info
title:Definition
color:255,130,178
Allgemein ist eine Matrix $A\in M_{m\times n}(\mathbb{R})$ eine rechteckige Anordnung von reellen Zahlen mit $m$ <b>Zeilen</b> und $n$ <b>Spalten</b>.

*Beispiele:*

$$\begin{align}\begin{pmatrix}36&72\\31&96\end{pmatrix}\in M_{2\times2}(\mathbb{R}), \begin{pmatrix}1&2&3\\4&5&6\end{pmatrix}&\in M_{2\times3}, \begin{pmatrix}1&2\\4&5\\5&7\\8&9\end{pmatrix}\in M_{4\times2}(\mathbb{R}),\\\\ \begin{pmatrix}36&72&425&\pi\end{pmatrix}\in M_{1\times4}(\mathbb{R})&,\begin{pmatrix}1\\4\\6\\8\end{pmatrix}\in M_{4\times1}(\mathbb{R})\end{align}$$
``` 

### Rechenoperationen auf der Menge $M_{2\times3}(\mathbb{R})$
<font style="color:MediumAquaMarine">Addition:</font>
$$\begin{pmatrix}1&2&3 \\ 4&5&6\end{pmatrix} + \begin{pmatrix}2&3&4 \\ 5&6&7\end{pmatrix}=\begin{pmatrix}3&5&7 \\ 9&11&13\end{pmatrix}$$
<font style="color:MediumAquaMarine">Nullelement:</font>
$$\begin{pmatrix}1&2&3 \\ 4&5&6\end{pmatrix} + \begin{pmatrix}0&0&0 \\ 0&0&0\end{pmatrix}=\begin{pmatrix}1&2&3 \\ 4&5&6\end{pmatrix}$$
<font style="color:MediumAquaMarine">Inverses Element:</font>
$$\begin{pmatrix}1&2&3 \\ 4&5&6\end{pmatrix} + \begin{pmatrix}-1&-2&-3 \\ -4&-5&-6\end{pmatrix}=\begin{pmatrix}0&0&0 \\ 0&0&0\end{pmatrix}$$

*Es gelten das Assoziativitäts- und Kommutationsgesetz bezüglich der Matrizenaddition.*

<font style="color:MediumAquaMarine">Skalare Multiplikation:</font>
Seien $\lambda,\kappa\in\mathbb{R}$:
$$\lambda\cdot\begin{pmatrix}1&2&3 \\ 4&5&6\end{pmatrix}=\begin{pmatrix}\lambda&2\lambda&3\lambda \\ 4\lambda&5\lambda&6\lambda\end{pmatrix}$$
Analog gelten für $A,B\in M_{2\times3}(\mathbb{R})$:
$$\begin{align}
\lambda\cdot(A+B)&=\lambda\cdot A+\lambda B\\
(\lambda+\kappa)\cdot A&=\lambda\cdot A+\kappa\cdot A\\
\lambda\cdot(\kappa\cdot A)&=(\lambda\cdot\kappa)\cdot A
\end{align}$$

```ad-tip
title:Erkenntnis
Die Menge der Matrizen mit $m$ <font style="color:MediumAquaMarine">Zeilen</font> $n$ <font style="color:MediumAquaMarine">Spalten</font> $=M_{m\times n}(\mathbb{R})$ bildet zusammen mit den Operationen <font style="color:MediumAquaMarine"><b>Matrizenaddition</b></font> und der <font style="color:MediumAquaMarine"><b>skalaren Multiplikation</b></font> einer Matrix mit einer reellen Zahl einen Vektorraum über $\mathbb{R}$.

<font style="opacity:0">a</font>

---

<font style="opacity:0">a</font>

<font style="color:MediumAquaMarine">Konvention</font>

Liegt eine Matrix $A\in M_{m\times n}(\mathbb{R})$ vor, so bezeichnet $a_{ij}$ das Matrix Element
- in der $i$<b>-ten Zeile</b> u n d
- in der $j$<b>-ten Spalte</b>
```

---

## Multiplikation von Matrizen — $\text{Matrix}\times\text{Matrix}$

```ad-info
title:Definition
color:255,130,178
Matrizen können mit einander multipliziert werden wenn die <font style="color:goldenrod">Anzahl der Spalten</font> von ${\color{goldenrod}A}$ der <font style="color:indianred">Anzahl der Zeilen</font> von ${\color{indianred}B}$ entspricht.

*Beispiel:*
$$\begin{align}
&\begin{pmatrix}
x&x&x&x&x\\
x&x&x&x&x\\
a_{i1}&a_{i2}&a_{i3}&a_{i4}&a_{i5}\\
x&x&x&x&x
\end{pmatrix}
\cdot
\begin{pmatrix}
x&x&b_{1j}&x&x\\
x&x&b_{2j}&x&x\\
x&x&b_{3j}&x&x\\
x&x&b_{4j}&x&x\\
x&x&b_{5j}&x&x
\end{pmatrix}=
\begin{pmatrix}
x&x&x&x&x&x\\
x&x&(A\cdot B)_{ij}&x&x&x\\
x&x&x&x&x&x\\
x&x&x&x&x&x\\
\end{pmatrix}\\\\
&\qquad\qquad A\text{ ist }4\times5\qquad\qquad\qquad\; B\text{ ist }5\times6\qquad\qquad\qquad\quad\; AB\text{ ist }4\times6
\end{align}$$

<font style="opacity:0">a</font> 

Das Matrixelement $(A\cdot B)_{ij}$ ist das *Skalarprodukt* zwischen der $i$-ten Zeile von A und der $j$-ten Zeile von B (jeweils als Vektor betrachtet), d.h. $$(AB)_{ij}=a_{i1}\cdot b_{1j}+a_{i2}\cdot b_{2j}+a_{i3}\cdot b_{3j}+a_{i4}\cdot b_{4j}+a_{i5}\cdot b_{5j}$$
``` 

Ist $A$ eine $m\times n$ Matrix und $B$ eine $n\times p$ Matrix können wir diese miteinander multiplizieren.
Das *Produkt* ist dann eine $m\times p$ Matrix$$\begin{pmatrix}m\text{ Zeilen}\\n\text{ Spalten}\end{pmatrix}\cdot\begin{pmatrix}n\text{ Zeilen}\\p\text{ Spalten}\end{pmatrix}=\begin{pmatrix}m\text{ Zeilen}\\p\text{ Spalten}\end{pmatrix}$$
**Extremfall:**
$1\times n$ Matrix mal $n\times1$ Matrix ergibt eine $1\times1$ Matrix, gleichzeitig ist das Ergebnis das Skalarprodukt.

```ad-tip
title:Hinweis
- Man multipliziert zwei Matrizen, in dem man alle Skalaprodukte "Zeile mal Spalte" berechnet.
- Quadratische Matrizen lassen sich nur dann miteinander multiplizieren, wenn sie dieselbe Größe haben.
```

```ad-warning
title:Regeln / Gesetzmäßigkeiten
- $A\cdot B \neq B\cdot A$ — damit nicht kommutativ
- $C\cdot(A+B)=C\cdot A+C\cdot B$ — linksseitig distributiv
- $(A+B)\cdot C=A\cdot C+B\cdot C$ — rechtsseitig distributiv
- $A\cdot(B\cdot C)=(A\cdot B)\cdot C$ — assoziativ

<font style="opacity:0">a</font> 

Für <font style="color:goldenrod">quadratische Matrizen (</font>${\color{goldenrod}n\times n}$<font style="color:goldenrod">)</font> gelten folgende Potenzgesetze:
- $A^{p}=A\cdot A\cdot\dots\cdot A$ — insgesamt $p$ mal
- $(A^{p})\cdot(A^{q})=A^{(p+q)}$
- $(A^{p})^{q}=A^{p\cdot q}$

```

Auf der Menge der *quadratischen $n\times n$ - Matritzen* gibt es die sogenannte Einheitsmatrix $I_n$, deren Diagonale nur aus $1$-en besteht und deren restliche Elemente sämtlich den Wert $0$ haben. Es gilt für alle Matrizen $A\in M_{n\times n}(\mathbb{R}):A\cdot I_{n}=I_{n}\cdot A=A$.

Damit gilt auf der Menge $M_{n\times n}(\mathbb{R})$ bezüglich der Matrizenmultiplikation zwar nicht das Kommutativgestz aber (immerhin) das Assoziativgestz und es gibt ein neutrales Element, die Einheitsmatrix $I_{n}$. Für die Gruppeneigenschaft von $(M_{n\times n}(\mathbb{R}), \cdot )$ fehlen noch die inversen Elemente (siehe [[8. Matrizen#Inversion von quadratischen Matrizen|inverse Matrix]]).

---

## Matrizen für elementare Zeilenumformungen

Die im Rahmen des [[7. Lineare Gleichungssysteme#Praxis des Gauss'schen Eliminiationsverfahren|Gauss'schen Eliminationsverfahren]] kennengelernten und zur Vereinfachung durchgeführten elementaren Zeilen-Opertionen
- Zeilentausch
- Vervielfachung einer Zeile
- Addition / Subtraktion des Vielfachen einer Zeile zu einer anderen Zeile
werden in die Welt der Matrizen abgebildet. Das baut auf der für ein LGS erkannten Struktur $$A\cdot x=b$$auf und betont den Stellnwert einer Matrix als wesentliches Strukturelement der linearen Algebra.

### Permutationsmatrizen
```ad-info
title:Definition
color:255,130,178
Eine *Zeilentausch Matrix* oder auch *Permutationsmatrix* $P_{ij}\in M_{m\times m}(K)$, ist die Matrix, die bei <b>Linksmultiplikation</b> mit einer Matrix $A\in M_{m\times n}(K)$ bewirkt, dass die Zeilen $i$ und $j$ von $A$ vertauscht werden.

Die *Permutationsmatrix* ist dabei folgendermaßen besetzt:
1. $p_{kk}=1$ für alle $k$ außer $k=i$ und $k=j$
2. $p_{ij}=p_{ij}=1$
3. alle sonstigen Matrixelemente haben dern Wert $0$

<font style="opacity:0">a</font> 

Weiter gilt:
1. eine Permutationsmatrix ist <b>immer</b> eine <b>quadratische Matrix</b>
2. eine Permutationsmatrix ist <b>immer invertierbar</b>. Die Inverse einer Permuationsmatrix ist die Permutationsmatrix selbst, d.h. es gilt $P_{ij}\cdot P_{ih}=I$
3. eine Permutationsmatrix ist weder eine obere noch eine untere Dreiecksmatrix
``` 

**Beispiele:**
1. Gegeben sei die Matrix $$A=\begin{pmatrix}1&2&3&4\\5&6&7&8\\9&10&11&12\\13&14&15&16\end{pmatrix}$$Wie lautet die Matrix $P_{13}$, mit der $A$ von links multipliziert werden muss, um die Zeilen $1$ und $3$ von $A$ zu vertauschen?$$P_{13}=\begin{pmatrix}0&0&1&0\\0&1&0&0\\1&0&0&0\\0&0&0&1\end{pmatrix}$$Denn es gilt:$$P_{13}\cdot A=\begin{pmatrix}0&0&1&0\\0&1&0&0\\1&0&0&0\\0&0&0&1\end{pmatrix}\cdot\begin{pmatrix}1&2&3&4\\5&6&7&8\\9&10&11&12\\13&14&15&16\end{pmatrix}=\begin{pmatrix}9&10&11&12\\5&6&7&8\\1&2&3&4\\13&14&15&16\end{pmatrix}$$
2. Die zu verändernde Matrix $A$ muss nicht quadratisch sein. Gegeben sei die Matrix $$A=\begin{pmatrix}1&2&3&4&5&6\\7&8&9&10&11&12\\13&14&15&15&17&18\end{pmatrix}$$Wie lautet die Matrix $P_{13}$, mit der $A$ von link multipliziert werden muss, um die Zeilen $1$ und $3$ von $A$ zu vertauschen?$$P_{13}=\begin{pmatrix}0&0&1\\0&1&0\\1&0&0\end{pmatrix}$$Denn es gilt:$$P_{13}\cdot A=\begin{pmatrix}0&0&1\\0&1&0\\1&0&0\end{pmatrix}\cdot\begin{pmatrix}1&2&3&4&5&6\\7&8&9&10&11&12\\13&14&15&15&17&18\end{pmatrix}=\begin{pmatrix}13&14&15&15&17&18\\7&8&9&10&11&12\\1&2&3&4&5&6\end{pmatrix}$$
3. Die zu verändernde Matrix kann auch ein Vektor sein. Die Zeilen $1$ und $2$ eines Vektors $\in \mathbb{R}^{3}$ sollen getauscht werden. Die zugehörige Zeilentausch-Matrix lautet:$$P_{12}=\begin{pmatrix}0&1&0\\1&0&0\\0&0&1\end{pmatrix}$$Denn es gilt (mit dem Beispiel-Vektor):$$V=\begin{pmatrix}1\\2\\3\end{pmatrix}\to P_{12}\cdot V=\begin{pmatrix}0&1&0\\1&0&0\\0&0&1\end{pmatrix}\cdot\begin{pmatrix}1\\2\\3\end{pmatrix}=\begin{pmatrix}2\\1\\3\end{pmatrix}$$

### Eliminationsmatrizen
````ad-info
title:Definition
color:255,130,178
Die *Eliminationsamtrix* $E_{ij}\in M_{m\times m}(K)$, die bei einer <b>Linksmultiplikation</b> mit einer Matrix $A\in M_{m\times n}(K)$ bewirkt, dass das $r$-fache der Zeile $i$ zur Zeile $j$ von $A$ addiert wird, ist folgendermaßen besetzt:
1. $e_{kk}=1$ für alle $k\in\{1,2,\dots,m\}$
2. $e_{ji}=r$
3. alle sonstigen Matrixelemente haben den Wert $0$

<font style="opacity:0">a</font> 

Weiter gilt:
1. eine Eliminationsmatrix ist <b>immer</b> eine <b>quadratische Matrix</b>
2. eine Eliminationsmatrix ist <b>immer invertierbar</b>; <i>Beispiel:</i> die Inverse

<font style="opacity:0">a</font> 

$\qquad$der Eliminationsmatrix $\begin{pmatrix}1&0&0&0\\0&1&0&0\\r&0&1&0\\0&0&0&1\end{pmatrix}$ ist die Matrix $\begin{pmatrix}1&0&0&0\\0&1&0&0\\-r&0&1&0\\0&0&0&1\end{pmatrix}$

<font style="opacity:0">a</font> 

```ad-tip
title:Bemerkung
Eine Eliminationsmatrix ist eine <b>unter</b> Dreiecksmatrix, wenn gilt $i\lt j$ gilt. Andernfalss ist sie eine <b>obere</b> Dreiecksmatrix.
```
```` 

**Beispiele:**
1. Gegeben sei die Matrix$$A=\begin{pmatrix}1&2&3&4\\5&6&7&8\\9&10&11&12\\13&14&15&16\end{pmatrix}$$Wie lautet die Matrix $E_{13}$, mit der $A$ von links multipliziert werden muss, um das 2-fache der Zeile $1$ zu Zeile $3$ von $A$ zu addieren?$$E_{13}=\begin{pmatrix}1&0&0&0\\0&1&0&0\\2&0&1&0\\0&0&0&1\end{pmatrix}$$Denn es gilt:$$E_{13}\cdot A=\begin{pmatrix}1&0&0&0\\0&1&0&0\\2&0&1&0\\0&0&0&1\end{pmatrix}\cdot\begin{pmatrix}1&2&3&4\\5&6&7&8\\9&10&11&12\\13&14&15&16\end{pmatrix}=\begin{pmatrix}1&2&3&4\\5&6&7&8\\11&14&17&20\\13&14&15&16\end{pmatrix}$$
 2. Die zu verändernde Matrix $A$ muss nicht quadratisch sein. Gegeben sei die Matrix $$A=\begin{pmatrix}1&2&3&4&5&6\\7&8&9&10&11&12\\13&14&15&15&17&18\end{pmatrix}$$Wie lautet die Matrix $E_{12}$, mit der $A$ von link multipliziert werden muss, um von Zeile $2$ das 7-fache der Zeile $1$ von $A$ zu abzuziehen?$$E_{12}=\begin{pmatrix}1&0&0\\-7&1&0\\0&0&1\end{pmatrix}$$Denn es gilt:$$E_{12}\cdot A=\begin{pmatrix}1&0&0\\-7&1&0\\0&0&1\end{pmatrix}\cdot\begin{pmatrix}1&2&3&4&5&6\\7&8&9&10&11&12\\13&14&15&15&17&18\end{pmatrix}=\begin{pmatrix}1&2&3&4&5&6\\0&-6&-12&-18&-24&-30\\13&14&15&15&17&18\end{pmatrix}$$
 3. Die zu verändernde Matrix kann auch ein Vektor sein. Die Zeilen $1$ und $2$ eines Vektors $\in \mathbb{R}^{3}$ sollen getauscht werden. Die zugehörige Zeilentausch-Matrix lautet:$$E_{12}=\begin{pmatrix}1&0&0\\-2&1&0\\0&0&1\end{pmatrix}$$Denn es gilt (mit dem Beispielvektor):$$V=\begin{pmatrix}1\\2\\3\end{pmatrix}\to E_{12}\cdot V=\begin{pmatrix}1&0&0\\-2&1&0\\0&0&1\end{pmatrix}\cdot\begin{pmatrix}1\\2\\3\end{pmatrix}=\begin{pmatrix}1\\0\\3\end{pmatrix}$$
  
### Multiplikationsmatrix
```ad-info
title:Definition
color:255,130,178
Die *Multiplikationsmatrix* $M_{i}\in M_{m\times m}(K)$, die bei <b>Linksmultiplikation</b> mit einer Matrix $A\in M_{m\times m}(K)$ bewirkt, dass eine Zeile $i$ durch ihr $r$-faches ersetzt wird, ist folgendermaßen besetzt:
1. $m_{kk}=1$ für alle $k\in\{1,2,\dots,m\}-\{i\}$
2. $m_{ii}=r$
3. alle sonstigen Matrixelemente haben den Wert $0$

<font style="opacity:0">a</font> 

Es gilt:
1. eine Multiplikationsmatrix ist <b>immer</b> eine <b>quadratische Matrix</b>
2. eine Multiplikationsmatrix ist <b>immer invertiverbar</b>; <i>Beispiel:</i> die Inverse der

<font style="opacity:0">a</font>

$\qquad$Multiplikationsmatrix $\begin{pmatrix}1&0&0&0\\0&1&0&0\\0&0&r&0\\0&0&0&1\end{pmatrix}$ ist die Matrix $\begin{pmatrix}1&0&0&0\\0&1&0&0\\0&0&\frac{1}{r}&0\\0&0&0&1\end{pmatrix}$
``` 

---

## Transposition von Matrizen

```ad-info
title:Definition
color:255,130,178
Ist Matrix $A$ wie folgt gegeben, dann gilt für die Transponierte Matrix $A^{T}$:

<font style="opacity:0">a</font> 

$$A=\begin{pmatrix}1&2&3\\0&0&4\end{pmatrix}\to A^{T}=\begin{pmatrix}1&0\\2&0\\3&0\end{pmatrix}$$
``` 

Aus der Matrix $A\in M_{m\times n}(\mathbb{R})$/$M_{m\times n}(\mathbb{C})$ entsteht die zu $A$ transponierte Matrix $A^{T}$, indem die komplette $i$-te Zeile (von links nach rechts) von $A$ in die komplette $i$-te Spalte (von oben nach unten) von $A^{T}$ überführt wird.

Es entsteht die zu $A$ adjungierte Matrix $A^{H}=\overline{A}^{T}$, wenn in $A^{T}$ zusätzlich jedes Element komplex konjugiert wird. Falls $A$ eine quadratische Matrix $\in M_{n\times n}(\mathbb{R})$/$M_{n\times n}(\mathbb{C})$ darstellt, sind folgende Spezialfälle zu betrachten:

```ad-info
title:Definitionen
color:255,130,178
- Eine Matrix $A\in M_{n\times n}(\mathbb{R})$ wird *symmetrisch* genannt, wenn gilt $A^{T}=A$, was gleichbedeutend ist mit der Bedingung $a_{ji}=a_{ij} \;\forall i,j\in\{1,2,\dots,n\}$.
- Eine Matrix $A\in M_{n\times n}(\mathbb{R})$ wird *schiefsymmetrisch* genannt, wenn gilt $A^{T}=-A$, was gleichbedeuent ist mit der Bedingung $a_{ji}=-a_{ij}\;\forall i,j\in\{1,2,\dots,n\}$.
- Eine Matrix $A\in M_{n\times n}(\mathbb{R})$ wird *hermitesch* genannt, wenn gilt $A=A^{H}:=\overline{A}^{T}$, was gleichbedeutend ist mit der Bedingung $a_{ji}=\overline{a}_{ij}\;\forall i,j\in\{1,2,\dots,n\}$.

<font style="opacity:0">a</font> 

**Anmerkung:**

Für das Standardskalarprodukt $u\cdot v=:(u,v)$ im reellen Vektorraum $\mathbb{R}^{n}$ gilt mit $u,v\in\mathbb{R}^{n}$ bzw. im komplexen Vektorraum $\mathbb{C}^{n}$ gilt mit $u,v\in\mathbb{C}^{n}$ ausgedrückt in Matrixschreibweise $$(u,v)=u^{T}\cdot \text{ bzw. }<u,v>=u^{T}\cdot\overline{v}$$
``` 

### Rechengesetze
1. $(A+B)^{T}=A^{T}+B^{T}$
2. $(A\cdot B)^{T}=B^{T}\cdot A^{T}$
3. $(A\cdot x)^{T}\cdot y=(x^{T}\cdot A^{T})\cdot y=x^{T}\cdot(A^{T}\cdot y)$
4. Mit einer gegebenen *symmetrischen* Matrix $A\in M_{n\times n}(\mathbb{R})$ gilt mit dem Standardskalarprodukt im $\mathbb{R}^{n}$ mit $u,v\in\mathbb{R}^{n}$$$(A\cdot u,v)=(A\cdot u)^{T}\cdot v=(u^{T}\cdot A^{T})\cdot v=u^{T}\cdot(A\cdot v)=(u,A\cdot v)$$
5. Mit einer gegebenen *hermiteschen* Matrix $A$ gilt mit dem semilinearen Standardskalarprodukt im $\mathbb{C}^{n}$ mit $u,v\in\mathbb{C}^{n}$$$<A\cdot u,v>=(A\cdot u)^{T}\cdot\overline{v}=(u^{T}\cdot A^{T})\cdot\overline{v}=u^{T}\cdot(\overline{A\cdot v})=<u,A\cdot v>$$
Man nennt eine Matrix $A$ mit den Eigenschaften *4*, *5* auch *selbstadjungiert* bezüglich des jeweils betrachteten Skalarprodukt.

---

## Inversion von quadratischen Matrizen

```ad-info
title:Definition
color:255,130,178
Eine quadratische Matrix $A$ heißt invertierbar, wenn es eine Matrix $A^{-1}$ gibt, so dass $$A^{-1}\cdot A=I\text{ und }A\cdot A^{-1}=I$$mit Einheitsmatrix $I$.

<font style="opacity:0">a</font> 

Matrix $A^{-1}$ heißt *Inverse zu A*.

$Ax=b$ multipliziert mit $A^{-1}$ ergibt:$$A^{-1}Ax=A^{-1}b\iff x=A^{-1}b$$

→ Was $A$ "bewirkt", macht $A^{-1}$ rückgängig.

<font style="opacity:0">a</font> 

<font style="color:indianred"><b>Nicht alle Matrizen haben eine Inverse.</b></font>
``` 

### Bemerkungen zu inversen Matrizen
1. Eine Matrix $A$ kann <b>keine zwei Inversen</b> haben
2. Ist $A$ invertierbar, so ist die einzige Lösung der Gleichungm $Ax=b$ durch $x=A^{-1}b$ gegeben
3. Angenommen es gibt eine <b>von Null verschiedenen</b> Vektor $x$ mit $Ax=0$
	Dann kann $A$ <b>keine Inverse</b> besitzen
	- Denn keine Matrix kann den Nullvektor $0$ wieder in $x$ überführen
	- ist folglich $A$ invertierbar, so ist die einzige Lösung $Ax=0$ der Nullvektor $x=0$
4. Eine $2\times2$-Matrix <b>ist invertierbar</b> genau dann, wenn $ad-bc\neq0$ gilt:$$\begin{pmatrix}a&b\\c&d\end{pmatrix}^{-1}=\frac{1}{ad-bc}\begin{pmatrix}d&-b\\-c&a\end{pmatrix}$$
5. Eine <b>Diagonalmatrix ist invertierbar</b>, wenn <b>keines der Diagonalelemente Null</b> ist.$$\text{Für }A=\begin{pmatrix}d_{1}&\;&\;\\\;&\ddots&\;\\\;&\;&d_{n}\end{pmatrix}\text{ gilt }A^{-1}=\begin{pmatrix}\frac{1}{d_{1}}&\;&\;\\\;&\ddots&\;\\\;&\;&\frac{1}{d_{n}}\end{pmatrix}$$

### Inverse eines Produkts
```ad-info
title:Definition
color:255,130,178
Sind $A$ und $B$ invertierbar, dann ist auch ihr <b>Produkt</b> $AB$ invertierbar. 

Die Inverse von $AB$ ist$$(AB)^{-1}=B^{-1}\cdot A^{-1}$$
``` 

```ad-tip
title:Sätze
1. Die Menge der invertierbaren Matrizen $\in M_{n\times n}(\mathbb{R})$ bildet mit der Operation 'Matrizenmultiplikation' eine Gruppe. Diese Gruppe ist allerdings nicht kommutativ.
2. Wenn $A\in M_{n\times n}(\mathbb{R})$ eine inverse Matrix besitzt, so besitzt auch $A^{T}$ eine Inverse und es gilt $(A^{T})^{-1}=(A^{-1})^{T}$.
	
	<font style="color:MediumAquaMarine">Beweis:</font> Es gilt$$A^{T}\cdot(A^{-1})^{T}=(A^{-1}\cdot A)^{T}=I^{T}=I\text{ und }(A^{-1})^{T}=(A\cdot A^{-1})^{T}=I^{T}=I$$ was den Satz beweist
```

### Berechnung der inversen Matrix $A^{-1}$ mittels Gauß-Jordan Elimination
Das Eliminationsverfahren berechnet aus der Gleichung $Ax=b$ die Lösung $x$ direkt.
Es ermöglicht uns gleichzeitig die Bestimmung von $A^{-1}$.

#### Vorgehen
Wir lösen die Gleichung $A\cdot A^{-1}=I$ spaltenweise.
- Produkt von $A$ mit der ersten Spalte von $A^{-1}$ (in diesem Fall $x_{1}$), soll die erste Spalte von $I$ ergeben: $Ax_{1}=e_{1}$
- Genauso funktioniert es mit den anderen Spalten von $A^{-1}\;\dots$

**Beispiel** für $A\in M_{3\times3}(\mathbb{R})$:
Es ergibt sich $$A\cdot A^{-1}=A\cdot(x_{1}\;x_{2}\;x_{3})=(A\cdot x_{1}\;A\cdot x_{3}\;A\cdot x_{3})=(e_{1}\;e_{2}\;e_{3})$$Um eine $3\times3$ - Matrix zu invertieren, müssen nun die 3 Gleichungen gelöst werden:$$\begin{align}
Ax_{1}&=e_{1}\\
Ax_{2}&=e_{2}\qquad\text{Vektoren }x_{1},x_{2}\text{ und }x_{3}\text{ sind dann die Spalten von }A^{-1}\\
Ax_{3}&=e_{3}
\end{align}$$Das Verfahren zur Bestimmung von $A^{-1}$ mittels Gauß-Jordan-Verfahren besteht nun darin, simultan 3 Lineare Gleichungssysteme zu lösen.

Während beim Gauss'schen Eliminationsverfahren mittels elementarer Zeilenumformungen eine vorgegebene Matrix in eine obere Dreiecks- oder Trapezform transformiert wird, transformiert die Gauss-Jordan-Elimination die Matrix in Diagonalform, so dass auf der Diagonalen ausschließlich $1$-en stehen.

<font style="color:indianred"><b>Anmerkung 1:</b></font>
Die Überführung einer Matrix in die Diagonalform ist nicht für jede Matrix möglich. Notwendiges und hinreichendes Kriterium ist, dass die Matrix invertierbar ist.

<font style="color:indianred"><b>Anmerkung 2:</b></font>
Man kann zeigen, dass eine invertierbare quadratische Matrix sich allein mittels Zeilenumformungen vom Typ 'Elimination' auf Diagonalform transformieren lässt - d.h. man braucht hierfür insbesondere keine Zeilenvertauschung.

**Beispiel:**
Es gilt die Matrix $A=\begin{pmatrix}2&3\\4&-5\end{pmatrix}$ zu der die Inverse bestimmt werden soll.
$$\begin{align}
&\left(\begin{array}{cc|cc}2&3&1&0\\4&-5&0&1\end{array}\right)
\to(\rm{II}-2\cdot\rm{I})\to\\\\
&\left(\begin{array}{cc|cc}2&3&1&0\\0&-11&-2&1\end{array}\right)
\to(-\frac{1}{11}\cdot\rm{II})\to\\\\
&\left(\begin{array}{cc|cc}2&3&1&0\\0&1&\frac{2}{11}&-\frac{1}{11}\end{array}\right)
\to(\rm{I}-3\cdot\rm{II})\to\\\\
&\left(\begin{array}{cc|cc}2&0&\frac{5}{11}&\frac{3}{11}\\0&1&\frac{2}{11}&-\frac{1}{11}\end{array}\right)
\to(\frac{1}{2}\cdot\rm{I})\to\\\\
&\left(\begin{array}{cc|cc}1&0&\frac{5}{22}&\frac{3}{22}\\0&1&\frac{2}{11}&-\frac{1}{11}\end{array}\right)
\end{align}$$
Die Inverse zu der Matrix $A=\begin{pmatrix}2&3\\4&-5\end{pmatrix}$ lautet demnach $A^{-1}=\begin{pmatrix}\frac{5}{22}&\frac{3}{22}\\\frac{2}{11}&-\frac{1}{11}\end{pmatrix}$

Bevor man sich in den Prozessablauf der Inversenbestimmung mittels Gauss-Jordan-Elimination begibt, ist es sinnvoll, zu prüfen, ob bzw. dass es zur vorgegebenen Matrix überhaupt eine inverse Matrix existiert. Hierfür gibt es verschiedene Kriterien <font style="color:grey">(diese werden im Verlauf der Vorlesung dargestellt)</font>.
Ein <b>Spezialfall</b>:
Für $2\times2$ - Matrizen gilt:
	Eine $2\times2$ - Matrix $\begin{pmatrix}a&b\\c&d\end{pmatrix}$ ist invertierbar genau dann, wenn gilt $ad-bc\neq0$

Für andere Fälle wird <font style="color:grey">(nach aktuellem Stand)</font> das Gauss-Jordan-Verfahren angewandt, gibt es keine Inverse, so kommt es während der Anwendung zu einem Muster, das Unlösbarkeit signalisiert (linke Seite $=0$ rechte Seite $\neq0$)

---

## Orthogonalität von Matrizen

```ad-info
title:Definition
color:255,130,178
Eine Matrix $A\in M_{n\times n}(\mathbb{R})$ heißt orthogonal, wenn für das Skalarprodukt gilt $(Au)\cdot(Av)=u\cdot v$ für beliebige Vektoren $u,v\in\mathbb{R}^{n}$.
``` 

```ad-tip
title:Satz
Für eine Matrix $A\in M_{n\times n}(\mathbb{R})$ sind folgende Aussagen äquivalent:
- $A$ ist orthogonal
- Die Spalten von $A$ bilden eine Orthonormalbasis
- Es gilt $A^{-1}=A^{T}$

<font style="opacity:0">a</font> 

<font style="color:MediumAquaMarine"><b>Plausibilisierung:</b></font> Entscheidend für die Erklärung des Satzes ist die Feststellung

<font style="opacity:0">a</font> 

$$(Ae_{i})\cdot(Ae_{j})=(i\text{-te Spalte von }A)\cdot(j\text{-te Spalte von }A)=e_{i}\cdot e_{j}=\begin{cases}1\text{ falls }i=j\\0\text{ falls }i\neq j\end{cases}$$
wenn $e_{i}$, $e_{j}$, $i$, $j\in\{1,2,\dots,n\}$ die Standard-Koordinatenvektoren des $\mathbb{R}^{n}$ darstellen.
```

---

## Abbildungen von quadratischen Matrizen

Betrachtet sei die Funktion: $f:M_{n\times n}(\mathbb{R})\to M_{n\times n}(\mathbb{R}), f:A\to f(A)$ 

**Beispiele:**
1. $f(x)=x^{4}+3x^{3}-5x^{2}+x+4$ — in diese Funktion kann man in der folgenden Weise Matrizen einsetzen und erhalten$$f:\begin{Bmatrix}&M_{n\times n}&\to&M_{n\times n}(\mathbb{R})\\&A&\to&A^{4}+3\cdot A^{3}-5\cdot A^{2}+4\cdot I_{n}\end{Bmatrix}$$Konstanten sind hier als Vielfache der passenden Einheitsmatrix $I_{n}$ zu verstehen

$f(x)=(x-1)\cdot(x+2)^{2}$ — auch in diese Funktion kann man in der folgenden Weise Matrizen einsetzen und erhalten $$f:\begin{Bmatrix}&M_{n\times n}&\to&M_{n\times n}(\mathbb{R})\\&A&\to&(A-I_{n})\cdot(A+2\cdot I_{n})^{2}\end{Bmatrix}$$Beispielsweise umgesetzt mit $n=2, A=\begin{pmatrix}1&2\\-1&3\end{pmatrix}$$$f\begin{pmatrix}\begin{pmatrix}1&2\\-1&3\end{pmatrix}\end{pmatrix}=\begin{pmatrix}0&2\\-1&2\end{pmatrix}\cdot\begin{pmatrix}3&2\\-1&5\end{pmatrix}^{2}=\begin{pmatrix}0&2\\-1&2\end{pmatrix}\cdot\begin{pmatrix}7&16\\-8&23\end{pmatrix}=\begin{pmatrix}-16&46\\-17&30\end{pmatrix}$$

---

## Matrix-Normen

Die Menge $M_{m\times n}(\mathbb{R})$ bildet zusammen mit den Operationen Matrizenaddition und der skalaren Multiplikation einer Matrix mit einer reellen Zahl einen Vektorraum über $\mathbb{R}$.

Dieser Vektorraum kann durch Einführung einer Matrixnorm zu einem normierten Vektorraum weiterentwickelt werden.
Normen für Matrizen $A\in M_{m\times n}(\mathbb{R})$:
1. Die *p-Matrixnorm*: $\Vert A\Vert=\left(\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n}\vert a_{ij}\vert^{p}\right)^\frac{1}{p}$
2. Die *Zeilensummennorm*: $\Vert A\Vert=\underset{i=1,2,\dots,m}{max}\sum\limits_{j=1}^{n}\vert a_{ij}\vert$
3. Die *Spaltensummennorm*: $\Vert A\Vert=\underset{j=1,2,\dots,m}{max}\sum\limits_{i=1}^{n}\vert a_{ij}\vert$

Für alle Matrixnormen gelten die 3 definierten Eigenschaften einer Norm:
1. $\Vert\lambda\cdot A\Vert=\vert\lambda\vert\cdot\Vert A\Vert$ für $\lambda\in\mathbb{R}, A\in M_{m\times n}(\mathbb{R})$
2. $\Vert A\Vert=0\iff A=0$ für $A\in M_{m\times n}(\mathbb{R})$
3. $\Vert A+B\Vert\lt\Vert A\Vert+\Vert B\Vert$ für $A,B\in M_{m\times n}(\mathbb{R})$

**Beispiel:**
Betrachtet sei die $2\times2$-Matrix $A=\begin{pmatrix}-5&1\\-3&-4\end{pmatrix}$. Zu berechnen sind:
1. die 2-Matrixnorm:$$\Vert A\Vert=\left(\sum\limits_{i=1}^{m}\sum\limits_{j=1}^{n}\vert a_{ij}\vert^{2}\right)^\frac{1}{2}=\sqrt{((-5)^{2}+1^{2})+((-3)^{2}+(-4)^{2})}=\sqrt{51}\approx7,14$$
2. die Zeilensummennorm:$$\Vert A\Vert=\underset{i=1,2,\dots,m}{max}\sum\limits_{j=1}^{n}\vert a_{ij}\vert=max(\vert-5\vert+\vert1\vert,\vert-3\vert+\vert-4\vert)=7$$
3. die Spaltensummennorm:$$\Vert A\Vert=\underset{j=1,2,\dots,m}{max}\sum\limits_{i=1}^{n}\vert a_{ij}\vert=max(\vert-5\vert+\vert-3\vert,\vert1\vert+\vert-4\vert)=8$$
Weitere Matrixnormen werden nicht behandelt.